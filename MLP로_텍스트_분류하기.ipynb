{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP로 텍스트 분류하기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhoanlee/MLP/blob/main/MLP%EB%A1%9C_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRihwEAul2G4"
      },
      "source": [
        "# 1. 케라스의 texts_to_matrix() 이해하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8IfjC9glrZC"
      },
      "source": [
        "texts = ['먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmt4RWFloBe"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4xqIcCqlsB4",
        "outputId": "e81f249f-60b4-4d09-80ae-74aae19d146d"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'바나나': 1, '먹고': 2, '싶은': 3, '사과': 4, '길고': 5, '노란': 6, '저는': 7, '과일이': 8, '좋아요': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRjr1wNeltIp",
        "outputId": "d5c10575-3b57-4b92-9859-2b71ad5a72e9"
      },
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'count'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 2. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ep-O1wXlwLx",
        "outputId": "cfc978d7-8476-49d5-a757-980e8eb5009a"
      },
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'binary'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR6BSbn0lyFo",
        "outputId": "2ce2cac0-f555-4dc3-d303-5a69fdf3d0dc"
      },
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'tfidf').round(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.   0.85 0.85 1.1  0.   0.   0.   0.   0.  ]\n",
            " [0.   0.85 0.85 0.85 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   1.43 0.   0.   0.   1.1  1.1  0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   1.1  1.1  1.1 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7eb55H7lziw",
        "outputId": "befb6eb4-f66d-42f0-b834-b95ab028ef6c"
      },
      "source": [
        "print(tokenizer.texts_to_matrix(texts, mode = 'freq').round(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.   0.33 0.33 0.33 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.33 0.33 0.33 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.5  0.   0.   0.   0.25 0.25 0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.33 0.33 0.33]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wscv65Sal4yZ"
      },
      "source": [
        "# 2. 20개 뉴스 그룹(Twenty Newsgroups) 데이터에 대한 이해"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y17M98EmVa7I",
        "outputId": "bc07e184-5f20-4fe4-b7ca-5f9b14a94368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 48.1 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6dEr7El0jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b130269-8cf7-4ba6-c906-49904f07c867"
      },
      "source": [
        "import gspread\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train = '/content/drive/My Drive/mbti/mbti_train.xlsx'\n",
        "test= '/content/drive/My Drive/mbti/mbti_test.xlsx'\n",
        "\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_data = pd.read_excel(train)\n",
        "test_data = pd.read_excel(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "jD1z25BBSbTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoA78baHmQZw",
        "outputId": "9901e791-2e38-4d63-c0d9-b31fa380eca7"
      },
      "source": [
        "train_data['내용'] = train_data['내용'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "train_data['내용'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "train_data = train_data.dropna(how='any') # Null 값 제거\n",
        "print('전처리 후 훈련용 샘플의 개수 :',len(test_data))\n",
        "\n",
        "test_data['내용'] = test_data['내용'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['내용'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\n",
        "\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','을','으로','자','에','와','한','하다',\n",
        "             '인프피','인픕','인픞','인프제','인티피','인팁','인팊','인티제','잇프피','잇픕','잇픞','잇프제',\n",
        "             '잇티피','잇팁','잇팊','잇티제','엔프피','엔픕','엔픞','엔프제','엔티피','엔팁','엔팊','엔티제',\n",
        "             '엣프피','엣픕','엣픞','엣프제','엣티피','엣팁','엣팊','엣티제']\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "train_text = []\n",
        "for sentence in tqdm(train_data['내용']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    train_text.append(stopwords_removed_sentence)\n",
        "\n",
        "test_text = []\n",
        "for sentence in tqdm(test_data['내용']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    test_text.append(stopwords_removed_sentence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 후 훈련용 샘플의 개수 : 2400\n",
            "전처리 후 테스트용 샘플의 개수 : 1990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6878/6878 [00:53<00:00, 128.42it/s]\n",
            "100%|██████████| 1990/1990 [00:13<00:00, 151.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkNdFO_DmVEY"
      },
      "source": [
        "\n",
        "train_label = train_data['label']\n",
        "test_label = test_data['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv2-ajxPmY-Q"
      },
      "source": [
        "max_words = 10000\n",
        "num_classes = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7IAke3umtVx"
      },
      "source": [
        "def prepare_data(train_data, test_data, mode): # 전처리 함수\n",
        "    tokenizer = Tokenizer(num_words = max_words) # max_words 개수만큼의 단어만 사용한다.\n",
        "    tokenizer.fit_on_texts(train_data)\n",
        "    X_train = tokenizer.texts_to_matrix(train_data, mode=mode) # 샘플 수 × max_words 크기의 행렬 생성\n",
        "    X_test = tokenizer.texts_to_matrix(test_data, mode=mode) # 샘플 수 × max_words 크기의 행렬 생성\n",
        "    return X_train, X_test, tokenizer.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUWZIIR6muPB"
      },
      "source": [
        "X_train, X_test, index_to_word = prepare_data(train_text, test_text, 'binary') # binary 모드로 변환\n",
        "y_train = to_categorical(train_label, num_classes) # 원-핫 인코딩\n",
        "y_test = to_categorical(test_label, num_classes) # 원-핫 인코딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy8kIG1Bmu_J",
        "outputId": "c20d1917-ebc4-4174-dec8-57a0c87d3f55"
      },
      "source": [
        "print('훈련 샘플 본문의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플 본문의 크기 : (6878, 10000)\n",
            "훈련 샘플 레이블의 크기 : (6878, 20)\n",
            "테스트 샘플 본문의 크기 : (1990, 10000)\n",
            "테스트 샘플 레이블의 크기 : (1990, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5Uiq9WlmvuZ",
        "outputId": "f4e67fb9-733f-42d1-cd1f-a299a3bb42e6"
      },
      "source": [
        "print('빈도수 상위 1번 단어 : {}'.format(index_to_word[1]))\n",
        "print('빈도수 상위 9999번 단어 : {}'.format(index_to_word[9999]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "빈도수 상위 1번 단어 : .\n",
            "빈도수 상위 9999번 단어 : 이어폰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQcE2khZmzoN"
      },
      "source": [
        "# 3. 다층 퍼셉트론(Multilayer Perceptron, MLP)을 사용하여 텍스트 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBExjIhSmwp5"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO5FJGfsm015"
      },
      "source": [
        "def fit_and_evaluate(X_train, y_train, X_test, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_shape=(max_words,), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1)\n",
        "    score = model.evaluate(X_test, y_test, batch_size=128, verbose=0)\n",
        "    return score[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqrlMT29m13x",
        "outputId": "9e5c4c99-a1a5-4999-c1ec-2f95ca33d859"
      },
      "source": [
        "modes = ['binary', 'count', 'tfidf', 'freq'] # 4개의 모드를 리스트에 저장.\n",
        "\n",
        "for mode in modes: # 4개의 모드에 대해서 각각 아래의 작업을 반복한다.\n",
        "    X_train, X_test, _ = prepare_data(train_text, test_text, mode) # 모드에 따라서 데이터를 전처리\n",
        "    score = fit_and_evaluate(X_train, y_train, X_test, y_test) # 모델을 훈련하고 평가.\n",
        "    print(mode+' 모드의 테스트 정확도:', score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "49/49 [==============================] - 3s 22ms/step - loss: 1.5281 - accuracy: 0.4861 - val_loss: 0.6994 - val_accuracy: 0.6119\n",
            "Epoch 2/5\n",
            "49/49 [==============================] - 0s 10ms/step - loss: 0.7305 - accuracy: 0.5901 - val_loss: 0.6776 - val_accuracy: 0.6003\n",
            "Epoch 3/5\n",
            "49/49 [==============================] - 0s 10ms/step - loss: 0.5705 - accuracy: 0.7076 - val_loss: 0.7083 - val_accuracy: 0.5799\n",
            "Epoch 4/5\n",
            "49/49 [==============================] - 0s 10ms/step - loss: 0.4483 - accuracy: 0.7948 - val_loss: 0.8036 - val_accuracy: 0.5785\n",
            "Epoch 5/5\n",
            "49/49 [==============================] - 0s 10ms/step - loss: 0.3308 - accuracy: 0.8604 - val_loss: 0.9743 - val_accuracy: 0.5538\n",
            "binary 모드의 테스트 정확도: 0.568844199180603\n",
            "Epoch 1/5\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 1.5334 - accuracy: 0.4871 - val_loss: 0.7008 - val_accuracy: 0.6308\n",
            "Epoch 2/5\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.7506 - accuracy: 0.5851 - val_loss: 0.6875 - val_accuracy: 0.5858\n",
            "Epoch 3/5\n",
            "49/49 [==============================] - 0s 10ms/step - loss: 0.5955 - accuracy: 0.6998 - val_loss: 0.7377 - val_accuracy: 0.5727\n",
            "Epoch 4/5\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.4716 - accuracy: 0.7830 - val_loss: 0.8474 - val_accuracy: 0.5465\n",
            "Epoch 5/5\n",
            "49/49 [==============================] - 0s 10ms/step - loss: 0.3534 - accuracy: 0.8498 - val_loss: 0.9552 - val_accuracy: 0.5610\n",
            "count 모드의 테스트 정확도: 0.5969849228858948\n",
            "Epoch 1/5\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 1.3014 - accuracy: 0.4916 - val_loss: 0.7095 - val_accuracy: 0.5799\n",
            "Epoch 2/5\n",
            "49/49 [==============================] - 1s 10ms/step - loss: 0.6492 - accuracy: 0.6620 - val_loss: 0.7165 - val_accuracy: 0.5858\n",
            "Epoch 3/5\n",
            "49/49 [==============================] - 1s 10ms/step - loss: 0.4489 - accuracy: 0.7939 - val_loss: 0.8561 - val_accuracy: 0.5494\n",
            "Epoch 4/5\n",
            "49/49 [==============================] - 0s 10ms/step - loss: 0.2945 - accuracy: 0.8780 - val_loss: 1.0048 - val_accuracy: 0.5509\n",
            "Epoch 5/5\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.2051 - accuracy: 0.9186 - val_loss: 1.2915 - val_accuracy: 0.5247\n",
            "tfidf 모드의 테스트 정확도: 0.5587939620018005\n",
            "Epoch 1/5\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.7875 - accuracy: 0.4924 - val_loss: 0.7079 - val_accuracy: 0.3765\n",
            "Epoch 2/5\n",
            "49/49 [==============================] - 1s 10ms/step - loss: 0.7576 - accuracy: 0.5005 - val_loss: 0.6855 - val_accuracy: 0.6294\n",
            "Epoch 3/5\n",
            "49/49 [==============================] - 1s 10ms/step - loss: 0.7062 - accuracy: 0.5405 - val_loss: 0.6891 - val_accuracy: 0.5610\n",
            "Epoch 4/5\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.6617 - accuracy: 0.6173 - val_loss: 0.7102 - val_accuracy: 0.4869\n",
            "Epoch 5/5\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.5979 - accuracy: 0.6853 - val_loss: 0.6984 - val_accuracy: 0.5610\n",
            "freq 모드의 테스트 정확도: 0.5969849228858948\n"
          ]
        }
      ]
    }
  ]
}